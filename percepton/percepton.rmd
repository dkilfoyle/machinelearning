---
title: "Percepton"
author: "Dean Kilfoyle"
date: "22 May 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(R6)
library(dplyr)
library(ggplot2)
source("utils.R")
source("Percepton.R")
```

## Percepton 

Weights are updated after each row of the training data

$$\Delta w = \eta\cdot\left(y_i\ -\ z\left(x_i\right)\right)\cdot x_i$$
Where z is the output function $z(x_i) = x_i * w_i + b$

```{r percepton, eval=F}
fit = function(X, y) {
  self$w = rep(0, ncol(X)) # initialize w to 0s
  self$b = 0
  self$ierrors = c()
  
  for (i in 1:self$n_iter) {
    errors = 0
    for (xi in 1:nrow(X)) {
      x = as.numeric(X[xi,])
      update = self$eta * (y[xi] - self$predict(x))
      self$w = self$w + (update * x)
      self$b = self$b + update
      errors = errors + as.integer(update != 0.0)
    }
    self$ierrors = append(self$ierrors, errors)
  }
  return(self)
},

net_input = function(x) {
  dotp = as.numeric(x %*% self$w)
  return(dotp + self$b)
},

predict = function(x) {
  return(ifelse(self$net_input(x)>=0.0,1,-1))
}
```

### Test

```{r}
test = iris %>% 
  slice(1:100) %>% 
  mutate(y = ifelse(Species=="setosa",-1,1))
```

```{r}
ppn = Percepton$new(eta=0.1, n_iter=10)
ppn$fit(test[,c(1,3)], test$y)
```

```{r}
p1 = ggplot(data.frame(error=ppn$ierrors, epoch=1:10), aes(x=epoch,y=error)) +
  geom_line()
p2 = plot_decision_regions(test[,c(1,3)], test$y, ppn, 0.1) +
  xlab("Sepal Length") +
  ylab("Petal Length")
multiplot(p1,p2,cols=2)
```


## Adaptive Linear Neuron

Weights are updated at the end of each iteration using the gradient of the cost function

$$\Delta w\ =\ -\eta\cdot\nabla J\left(w\right)$$

The cost function J is the Sum of Squared Errors

$$J\left(w\right)\ =\ \frac{1}{2}\cdot\sum_{\left(i=1\right)}^n\left(y_i\ -\ \theta\left(z\left(x_i\right)\right)\right)^2$$

Where the activation function $\theta$ is the identity function $\theta(z)=z$

The gradient $\nabla$ of the cost function is the partial derivative of the cost function with respect to each weight

$$\nabla J\left(w_j\right)\ =\ \frac{\partial J}{\partial w_j}=\sum_i^{ }\left(y_i\ -\theta\left(z\left(x_i\right)\right)\right)x_{ij}$$


```{r adaline, eval=F}
fit = function(X, y) {
  self$w = rep(0, ncol(X)) # initialize w to 0s
  self$b = 0
  self$cost = c()
  
  for (i in 1:self$n_iter) {
    output = apply(X, 1, self$net_input)
    errors = (y - output)
    # weight change = -n * gradient of cost
    # gradient of sum squared error is -sum(y-output)xi
    self$w = self$w + self$eta * (t(X) %*% errors)
    self$b = self$b + self$eta * sum(errors)
    cost = sum((errors * errors)) / 2.0
    self$cost = append(self$cost, cost)
  }
  return(self)
}
```

### Test

```{r}
test = iris %>%
  slice(1:100) %>%
  mutate(y = ifelse(Species=="setosa",-1,1))

ppn2 = AdalineGD$new(eta=0.01, n_iter=15)
ppn2$fit(scale(test[,c(1,3)]), test$y)

p1 = ggplot(data.frame(cost=ppn2$cost, epoch=1:15), aes(x=epoch,y=cost)) +
  geom_line()
p2 = plot_decision_regions(scale(test[,c(1,3)]), test$y, ppn2, 0.1) +
  xlab("Sepal Length") +
  ylab("Petal Length")
multiplot(p1,p2,cols=2)

```



