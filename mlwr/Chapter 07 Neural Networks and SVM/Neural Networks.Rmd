---
title: "Decision Trees and Rules"
author: "Dean Kilfoyle"
date: "6 June 2016"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "all" } }
});
</script>

## Neural Networks

Take a bunch of predictor variables, multiple each by an initially random weighting factor, and a bias and add all the weighted predictors (dendrites) together. 

$$y(x) = f(\sum_{i=1}^nw_ix_i+b)$$
Feed the summated weighted value into a sigmoid output function and this becomes the neuron output.

$$\sigma = f(x)=\frac{1}{1+e^{-x}}\label{sigma}$$

```{r}
x =(-100:100)/10
data.frame(x=x, y=1/(1+exp(-x))) %>% 
  ggplot(aes(x=x,y=y)) +
  geom_line()
```


If we store the weights for each layer in a matrix with the counter-intuitaive ordering of $$w^l_{jk}\nonumber$$

Where k represents the kth neuron in l-1 layer and j represents the jth neuron in the l layer. Using this arrangement we can use matrix algebra to computer equations 1 and 2:

$$a^{l} = \sigma(w^l a^{l-1}+b^l)$$

The neuron outputs will then usually act is the predictor variables for a second layer of hidden neurons. The weighting/summating/output process is then repeated producing a final output

The performance of the algorithm with given set of n w's and b's across the whole training set x is measured by the cost function

$$c(w,b) = \frac{1}{2n}\sum_x{||y(x)-a||^2} \label{cost}$$

where y(x) is the vector of desired outputs and a is the vector of network outputs. C is thus the Mean Squared Difference.

Now we want to find the best combinations of w's and b's that minimises the cost function. Error reduction is acheived by **gradient descent**.

## Gradient Descent

We want to find the multidimensional gradient vector of the cost function. Each dimension will be a specific w or b. If we then go backwards (negative amount) down this vector a small amount in every dimension then we must have reduced the cost.

$$\Delta C \approx \frac{\partial C}{\partial w_1} \Delta w_1 + \frac{\partial C}{\partial w_2} \Delta w_2 + ...\nonumber$$

And if we define $\nabla C$ as the gradient vector...

 $$   \nabla C \equiv \left(\frac{\partial C}{\partial w_1}, \ldots, \frac{\partial C}{\partial w_m}\right)^T \nonumber$$

Then we can manipulate this to get $\Delta v$ which is the desired change in the w,b variables:

$$  \Delta C \approx \nabla C \cdot \Delta v \nonumber$$

$$\Delta v = -\eta \nabla C \tag{7} \nonumber$$
$$v \rightarrow v' = v -\eta \nabla C $$

### Stochastic Gradient Descent

When the number of training inputs is large then it can take a long time to calculate $\nabla C$. Because C is the Mean Squared Distance we can use a smaller random mini-batch sample to estimate C.

$$\nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}} \nonumber$$

and thus

$$w_k \rightarrow w_k' = w_k-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \label{deltaw}$$
$$b_l \rightarrow b_l' = b_l-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l} \label{deltab}$$
  
## Back Propogation

Back propogation lets us compute $\partial C / \partial w^l_{jk}$ from the cost function $C = \frac{1}{2} \sum_j (y_j-a^L_j)^2$

We use some intermediary quantities:

$$z^l \equiv w^l a^{l-1}+b^l$$

Quote neuralnetworksanddeeplearning.com:
Consider a demon which sits at the jth neuron in layer l. As the input to the neuron comes in, the demon adds a little change $\Delta z^l_j$ to the neuron's weighted input, so that instead of outputing $\sigma(z^l_j)$, the neuron instead outputs $\sigma(z^l_j + \Delta z^l_j)$. This changes propagates through the later layers in the network finally causing the overall cost to change by $\frac{\partial C}{\partial z^l_j} \Delta z^l_j$

We use this to define the neurons error term

$$\delta^l_j = \frac{\partial C}{\partial z^l_j}$$

### Error terms of the output layer

We can manipulate the generic neuron error equation to produce the error terms of the output layer

$$  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) $$
where $\frac{\partial C}{\partial a^L_j} = (a_j - y_)$ from partial derivates of C.

### Layer errors in terms of the error in the next layer

We can further manipulate to get $\delta^l_j$ from the error terms of the **next** layer

$$  \delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j) $$

### Rate of change of cost with respect to bias

A neurons error term is exactly equal to the rate of change of cost with respect to bias for that neuron. 

$$\frac{\partial C}{\partial b^l_j} = \delta^l_j$$
This is helpful because we already know the error term from the two other equations above

### Rate of change of cost with respect to any weight

This is what we were always trying to get to, an equation for pdC/pdw - which we needed for the gradient descent method of reducing the cost.

$$ \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$$

### Algorithm

1. Input. Set the activation a^1 for the input layer
2. Feedforward. For each layer l=2,3...L compute
    - $z^l = w^l a^{l-1} + b^l$
    - $a^{l} = \sigma(z^{l})$
3. Output error. Compute the vector
    - $\delta^{L} = \nabla_a C \odot \sigma'(z^L)$
4. Backpropagate the error. For each l=L-1, L-2,... compute
    - $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})$
5. Output the gradient of the cost function at each wjk and bj
    - $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$
    - $\frac{\partial C}{\partial b^l_j} = \delta^l_j$
6. Gradient descent. Update the weights and biases according to $\ref{deltaw}$ and $\ref{deltab}$:
    - $w^l \rightarrow  w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T$
    - $b^l \rightarrow b^l-\frac{\eta}{m}  \sum_x \delta^{x,l}$


## R Code

Adapted from python code [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)


```{r}
set.seed(12345)
sigmoid = function(z) return(1.0/(1.0+exp(-z)))
sigmoid.prime = function(z) return(sigmoid(z)*(1-sigmoid(z)))

sizes=c(3,4,1)
num.layers=length(sizes)

# generate a vector for each layer containing biases for each neuron. Exclude 1st layer which is input only and doesn't need a bias
biases = lapply(sizes[-1], rnorm)

# generate a matrix for each layer where each entry is the weight between the row neuron and the col neuron from the layer before
mdims = rbind(head(sizes,-1), sizes[-1])
weights = lapply(1:ncol(mdims), function (y) {
  matrix(rnorm(mdims[1,y]*mdims[2,y]), nrow=mdims[2,y], ncol=mdims[1,y])
})

feedForward = function(a) {
  # feed the activations from the previous layer into the neurons of the next layer
  for (i in 1:(num.layers-1)) {
    a = sigmoid((weights[[i]] %*% a)+biases[[i]])
  }
  return(a)
}

# Stochastic gradient descent
# Do gradient descent on epochs of mini.batch.size instead of the entire test data
# This makes calculating the cost gradient much less intensive
SGD = function(training.data, epochs, mini.batch.size, eta, test.data=NULL) {
  if (!(is.null(test.data)))
    n.test=length(test.data)
  n = length(training.data)
  
  for (j in 1:epochs) {
    training.data.shuffledindices = sample(1:length(training.data))
    training.data = training.data[c(training.data.shuffledindices)]
    
    mini.batches = lapply(seq(0,n,mini.batch.size), function(x) {
      training.data[x:x+mini.batch.size]
    })
    
    for (k in 1:length(mini.batches)) {
      update.mini.batch(mini.batches[[k]], eta)
    }
    
    cat("Epoch ", j, "complete\n")
  }
}  

# Update network weights and biases by applying gradient descent backpropogation to a single minibatch
update.mini.batch = function(mini.batch, eta) {
  
  # Zero the gradient vectors using the same shape as the source 
  nabla.b.sum = lapply(biases, function(x) x*0)
  nabla.w.sum = lapply(weights, function(x) x*0)
    
  # calculate the sum over mini batch of the nablas
  for (i in 1:nrow(mini.batch)) {
    
    # backpropagation ==============================================================
    deltas = backprop(mini.batch[i,1], mini.batch[i,2]) # calculate the nabla_deltas
  
    nabla.b.sum = nabla.b.sum + deltas$nabla.b
    nabla.w.sum = nabla.w.sum + deltas$nabla.w
  }
  
  # see eq 10 and 11
  # new_w = old_w + eta/m * batchsum(nabla.w)
  weights <<- lapply(weights, function(w) w-(eta/nrow(mini.batch))*nabla.w.sum)
  biases <<- lapply(biases, function(b) b-(eta/nrow(mini.batch))*nabla.b.sum)
  
}

# return a list of nabla.b and nabla.w which are the pC/pw and pC/pb calculated from the activations and the errors
backprop = function(x, y) {

  # Zero the gradient vectors using the same shape as the source 
  nabla.b = lapply(biases, function(x) x*0)
  nabla.w = lapply(weights, function(x) x*0)
  
  # feedforward
  activation = x # input values
  activations = list(x) # list to store all the activations layer by layer
  zs = list() # list to store all the zs layer by layer, where z = wa+b
  
  # L = last layer ie the output layer
  L = num.layers-1 # because first layer is input only
  
  for (l in 1:L) {
    z = (weights[[l]] %*% activation) + biases[[l]]
    zs = append(sz, z)
    activation = sigmoid(z)
    activations = append(activations, activation)
  }
  
  # Now the back propagation
  
  # output error term = costderivative * sigmoid derivative(z)
  errorL = (activations[[L]]-y) * simgoid.prime(zs[[L]])
  
  nabla.b[[L]]=errorL
  nabla.w[[L]]=errorL %*% t(activations[[L-1]])
  
  errorl = errorL
  
  for (l in (L-1):1) {
    z = zs[[l]]
    # calculate errorl from errorl+1
    errorl = (t(weights[[l+1]]) %*% errorl) * sigmoid.prime(z)
    nabla.b[[l]] = errorl
    nabla.w[[l]] = errorl %*% t(activations[[l-1]])
  }
  
  return(list(nabla.w=nabla.w, nabla.b=nabla.b))
}
```


## Testing
 
```{r}
# iris.scale = scale(iris[,1:4])
# species.n = as.integer(iris$Species)
# training=list()
# for (i in 1:nrow(iris.scale)) {
#   y=c(0,0,0)
#   y[species.n[i]]=1
#   training[[i]] = list(
#     as.vector(iris.scale[i,1:4]),
#     y
#   )
# }

training=list()
training[[1]]=list(c(0,0,1),c(0))
training[[2]]=list(c(0,1,1),c(1))
training[[3]]=list(c(1,0,1),c(1))
training[[4]]=list(c(1,1,1),c(0))

SGD(training, 10, 4, 0.3)
```


## R Library

```{r loaddata}
concrete <- read.csv("concrete.csv")

# custom normalization function
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

# apply normalization to entire data frame
concrete_norm <- as.data.frame(lapply(concrete, normalize))

# create training and test data
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

### Train the neuralnet model
 
```{r}
library(neuralnet)

# simple ANN with only a single hidden neuron
set.seed(12345) # to guarantee repeatable results
concrete_model <- neuralnet(formula = strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = concrete_train)

# visualize the network topology
plot(concrete_model)
```

### Evaluating model performance

```{r}
# obtain model results
model_results <- compute(concrete_model, concrete_test[1:8])
# obtain predicted strength values
predicted_strength <- model_results$net.result
# examine the correlation between predicted and actual values
cor(predicted_strength, concrete_test$strength)
```

