<div id="step-1-feed-forward" class="section level3">
<h3>Step 1: Feed Forward</h3>
<p>Each neuron in layer l receives weighted input from every neuron in layer l-1</p>
<p><span class="math display">\[z(x) = \sum_{i=1}^nw_ix_i+b\]</span></p>
<p>The weight matrix is stored as w_oi (output, input) so that the above equation can be completed for each neuron in a single dot product</p>
<pre><code>    I1  I2         
H1  w   w   dot  I1   =   H1z
H2  w   w        I2       H2z</code></pre>
<p>The output z is then feed into an activation function of several choices.</p>
<p>Sigmoid function</p>
<p><span class="math display">\[\sigma = \frac{1}{1+e^{-x}}\]</span></p>
<p>This squeezes all outputs into range 0..1 and prevents extreme values from affecting the output.</p>
<p><img src="data:image/png;base64,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" width="192" /></p>
<p>The derivative of the sigmoid function is:</p>
<p><span class="math display">\[S'(x) = S(x) * ( 1.0 - S(x) )\]</span></p>
</div>
<div id="step-2-back-propogation" class="section level3">
<h3>Step 2: Back propogation</h3>
<p>Starting with the output layer the error is calculated for each neuron (where i is the expected:</p>
<p><span class="math display">\[E = (actual-expected)\]</span></p>
<p>Gradient descent calculates the gradient of the Error function for the given set of weights.</p>
<p><span class="math display">\[\frac{ \partial E}{\partial w_{(ik)}} = \delta_k \cdot o_i\]</span> Where the deltaâ€™s are:</p>
<p><span class="math display">\[\delta_i = \begin{cases}-E f'(z_i) &amp; \mbox{, output nodes}\\ f'(z_i) \sum_k w_{ki}\delta_k &amp; \mbox{, interier nodes}\\ \end{cases}\]</span></p>
<p>where i is each neuron in the current layer k is each neuron in the backwards prior layer (ie current layer + 1)</p>
<p>Note interior deltas are calculated from the delta in the backwards prior layer.</p>
<p>Weights are updated to go down the gradient (ie to minimize the error)</p>
<p><span class="math display">\[w_k = w_k-\eta \sum_j \frac{\partial E_{X_j}}{\partial w_k} \label{deltaw}\]</span></p>
</div>
