---
title: "Naive Bayes"
author: "Dean Kilfoyle"
date: "6 June 2016"
output: html_notebook
---

## Setup

Load in some txt messages classifed by spam/ham and convert to document sparse matrix

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loaddata}
sms_raw <- read.csv("Chapter04/sms_spam.csv", stringsAsFactors = FALSE)
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type)
```

```{r textclean}
library(tm)
library(SnowballC) # for steming

sms_corpus <- VCorpus(VectorSource(sms_raw$text))

# clean up the corpus using tm_map()
# simplify messages by lower case, no numbers, remove stop words, remove punctuation, simplify words to stem eg learning to learn
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers) # remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords()) # remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation) # remove punctuation
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace) # eliminate unneeded whitespace

lapply(sms_corpus[1], as.character)
lapply(sms_corpus_clean[1], as.character)
```


```{r dtms}
# create a document-term sparse matrix: convert the messages into a matrix of 1 row per message and 1 col per word
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)

# creating training and test datasets
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]

# also save the labels
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels  <- sms_raw[4170:5559, ]$type

# save frequently-appearing terms to a character vector
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)

# create DTMs with only the frequent terms - remove all words that occur in less than 5 messages
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]

# convert counts to a factor
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

# apply() convert_counts() to columns of train/test data
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)

sms_train[1:10,2:10]
```

## Bayes Theorem

Posterior probability calculated from the likelihood

$$ P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)} $$

eg

$$ P(Spam \mid free) = \frac{P(free \mid Spam) \, P(Spam)}{P(free)} $$

### P(Spam)

```{r}
pspam = prop.table(table(sms_raw$type))
pspam
```

### P(free | Spam)

```{r}
pfreespam = prop.table(table(sms_train[sms_train_labels=="spam", "free"]=="Yes"))
pfreespam
```

### P(free)

```{r}
pfree=prop.table(table(sms_train[, "free"]=="Yes"))
pfree
```

### Answer

```{r}
(pfreespam[2] * pspam[2]) / pfree[2]
```

## Naive Bayes

Disadvantage: Assumes all features in the dataset have equal important = usually not the case.

## Example: spam detector
 
```{r}
# word cloud visualization
# library(wordcloud)
# wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
```

### Training a model on the data 

```{r}
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
```

### Evaluating model performance

```{r}
sms_test_pred <- predict(sms_classifier, sms_test)

library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
           dnn = c('predicted', 'actual'))
```


