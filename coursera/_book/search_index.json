[
["support-vector-machine.html", "8 Support Vector Machine 8.1 SVM Cost Function 8.2 Optimisaton target 8.3 Large Margin Optimisation 8.4 Kernels 8.5 Example", " 8 Support Vector Machine 8.1 SVM Cost Function 8.1.1 Logistic Regression \\[\\begin{align*}&amp; h_\\theta (x) = g ( \\theta^T x ) \\newline \\newline&amp; z = \\theta^T x \\newline&amp; g(z) = \\dfrac{1}{1 + e^{-z}}\\end{align*}\\] g = function(z) { 1 / (1 + exp(-z)) } data.frame(x=seq(from=-5, to=5, by=0.1)) %&gt;% mutate(g=g(x)) %&gt;% ggplot(aes(x,y=g)) + geom_line() + geom_vline(xintercept = 0, linetype=2) + ggtitle(TeX(&quot;$h_{\\\\theta}(x)=g(z)$&quot;)) + xlab(TeX(&quot;$z=\\\\theta^Tx&quot;)) If y=1 we want z &gt;&gt; 0 If y=0 we want z &lt;&lt; 0 8.1.2 Logistic Cost Function \\[\\mathrm{Cost}(h_\\theta(x),y) = - (y \\; \\log h_\\theta(x) + (1 - y) \\log(1 - h_\\theta(x))\\] 8.1.2.1 Cost if y=1 If y=1 the second term drops out leaving \\[cost = -\\log \\dfrac{1}{1+e^{-z}}\\] data.frame(z=seq(from=-3, to=3, by=0.1)) %&gt;% mutate(cost=-log(g(z))) %&gt;% ggplot(aes(x=z,y=cost)) + geom_line() Thus to reduce cost make z as big as possible. SVM will tweak this to produce straight line down to 1, then flat line at y=0 to right. 8.1.2.2 Cost if y=0 If y = 0 then the first term drops out leaving \\[cost = -\\log \\left( 1- \\dfrac{1}{1+e^{-z}}\\right)\\] data.frame(z=seq(from=-3, to=3, by=0.1)) %&gt;% mutate(cost=-log(1-g(z))) %&gt;% ggplot(aes(x=z,y=cost)) + geom_line() Thus if y=0 then to minimize cost want to make z as negative as possible. 8.1.3 SVM Cost Function Reparameterised logisitic regression function: remove 1/m replace lambda with C = 1/lambda replace the log sigmoids with cost_1 and cost_0 \\[\\displaystyle\\mathop{\\mbox{min}}_\\theta\\ C\\left[\\sum_{i=1}^{m}y^{(i)}\\text{cost}_1(\\theta^Tx^{(i)}) + (1-y^{(i)}) \\text{cost}_0(\\theta^Tx^{(i)})\\right]+\\frac{1}{2}\\sum_{j=1}^n\\theta^2_j\\] 8.2 Optimisaton target Minimise \\(\\frac{1}{2} \\sum_{1}^n{\\theta^2}\\) Such that \\(\\theta^Tx^{(i)} \\geq 1\\) if \\(y^{(i)}=1\\) Such that \\(\\theta^Tx^{(i)} \\leq -1\\) if \\(y^{(i)}=0\\) 8.2.1 Inner product optimisation :Inner product: \\(u^T v\\) = length along u to the perpendicular projection of v onto u. \\[u^T * v = p * ||u||\\] If we consider \\(\\theta\\) to define the normal of the separation boundary then \\(\\theta^Tx^{(i)}\\) becomes the perpendicular distance of \\(x^{(i)}\\) to the decision boundary. Also \\(\\sum \\theta^2\\) can be rewritten as \\(\\sqrt{(\\theta_1^2 + \\theta_2^2)}^2 = ||\\theta||^2\\) Therefore optimisation target becomes: Minimise \\(\\theta\\) for \\(\\frac{1}{2} ||\\theta||^2\\) Such that \\(p * ||\\theta|| \\geq 1\\) if y = 1 Such that \\(p * ||\\theta|| \\leq -1\\) if y = 0 8.3 Large Margin Optimisation We want to produce a decision boundary that maximises the distance of each point to the boundary, ie maximises p = distance along boundary normal to point. Theta defines the normal of the decision boundary. Project each xi onto theta vector using inner product and measure the distance from origin to projected point = p &lt;= maximise this. By maximising p we can minimise theta which will simultaneously satisfy optimisation targets 1 and 2. 8.3.1 Functional vs Geometric \\(\\gamma^{(i)}\\) is the un-scaled distance from \\(x^{(i)}\\) to the decision boundary. w is the un-scaled normal vector of the decision boundary. smvMargine \\(\\gamma^{(i)}\\) can be expressed in terms of w and x because: Point B = \\(x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{||w||}\\) But because B lies on the decision boundary it must satisfy equation \\(w^Tx + b = 0\\). Combining and rearranging yields: \\[\\gamma^{(i)}=y^{(i)} \\left(\\left(\\frac{w}{||w||}\\right)^Tx^{(i)} + \\frac{b}{||w||}\\right)\\] Where y(i) {-1,1} and indicates whether on positive or negative side of decision boundary. 8.3.2 Optimisation Target min for \\(\\gamma\\), w, b \\(\\frac{1}{2}||w||^2\\) s.t. \\(y^{(i)} \\left(w^Tx^{(i)} + b\\right) \\geq 1\\) for i=1..m 8.4 Kernels Instead of features x_1, x_2, x_3 calculate the distance/similarity of each xi to m landmarks l. Landmarks are set xi. Use Kernel functions to calculate the degree of similarity. 8.4.1 Gaussian \\[f_1 = \\exp(-\\frac{||x-l^1||^2}{2\\sigma^2})\\] To predict y=1 \\(\\theta^T*f &gt; 0\\) Increase sigma to broaden the similarity reach of each landmark. This helps prevent overfitting. 8.5 Example 8.5.1 Load data ex6data1 = readMat(&quot;ex6/ex6data1.mat&quot;) dat1 = data.frame(cbind(ex6data1$X, ex6data1$y)) colnames(dat1) = c(&quot;x1&quot;,&quot;x2&quot;,&quot;y&quot;) dat1 %&gt;% ggplot(aes(x1,x2, group=factor(y))) + geom_point(aes(color=factor(y), shape=factor(y)), size=2) "]
]
