[
["logistic-regression.html", "4 Logistic Regression 4.1 Model representation 4.2 Cost 4.3 Gradient Descent 4.4 Exercise 4.5 Optimised version 4.6 Regularisation 4.7 Prediction", " 4 Logistic Regression library(ggplot2) library(latex2exp) library(shiny) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union source(&quot;utils.R&quot;) 4.1 Model representation \\[\\begin{align*}&amp; h_\\theta (x) = g ( \\theta^T x ) \\newline \\newline&amp; z = \\theta^T x \\newline&amp; g(z) = \\dfrac{1}{1 + e^{-z}}\\end{align*}\\] ## Sigmoid Function g = function(z) { 1 / (1 + exp(-z)) } data.frame(x=seq(from=-5, to=5, by=0.1)) %&gt;% mutate(y=g(x)) %&gt;% ggplot(aes(x,y)) + geom_line() + geom_vline(xintercept = 0, linetype=2) 4.2 Cost \\[\\begin{align*}&amp; J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)}) \\newline &amp; \\mathrm{Cost}(h_\\theta(x),y) = -\\log(h_\\theta(x)) \\; &amp; \\text{if y = 1} \\newline &amp; \\mathrm{Cost}(h_\\theta(x),y) = -\\log(1-h_\\theta(x)) \\; &amp; \\text{if y = 0}\\end{align*}\\] Which can be expressed in a single equation as \\[\\mathrm{Cost}(h_\\theta(x),y) = - y \\; \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x))\\] ## Loading required package: grid 4.2.1 Gradient of Cost Function Is the partial derivative and is identical to the linear regression derivative \\[\\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\\] 4.2.2 Vectorized \\[\\begin{align*} h &amp;= g(X\\theta)\\newline J(\\theta) &amp;= \\frac{1}{m} \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right)\\newline \\dfrac{\\partial}{\\partial \\theta_J} &amp;= \\frac{1}{m} X^{T} (g(X \\theta ) - \\vec{y}) \\end{align*}\\] cost = function(theta, X, y) { m=length(y) z = X %*% theta h = g(z) 1/m * (t(-y) %*% log(h) - (t(1-y) %*% log(1-h))) } gradient = function(theta, X, y) { 1/m * t(X) %*% (g(X %*% theta) - y) } 4.3 Gradient Descent Calculating the partial derivative of the above cost function gives an identical equation to the multiple linear regression delta: \\[ \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\\] ### Vectorized \\[\\theta := \\theta - \\frac{\\alpha}{m} X^{T} (g(X \\theta ) - \\vec{y})\\] gradientDescent = function(X, y, theta=rep(0, ncol(X)), alpha=0.1, num_iters=4000) { m = length(y) J_history = rep(0, num_iters+1) theta_history = matrix(nrow=num_iters+1, ncol=ncol(X)) J_history[1] = cost(theta, X, y) theta_history[1, ] = theta alpha_over_m = alpha / m for (iter in 1:num_iters) { deltas = alpha_over_m * t(X) %*% (g(X %*% theta) - y) theta = theta - deltas J_history[iter+1] = cost(theta, X, y) theta_history[iter+1, ] = theta } return(list(theta=theta, theta_history = theta_history, J_history=J_history)) } 4.4 Exercise X = as.matrix(cbind(rep(1, m), ex2data1[, 1:2])) colnames(X) = c(&quot;x0&quot;,&quot;x1&quot;,&quot;x2&quot;) n=ncol(X) y = ex2data1[,3] theta=rep(0, n) cost(theta, X, y) ## [,1] ## [1,] 0.6931472 gradient(theta,X,y) ## [,1] ## x0 -0.10000 ## x1 -12.00922 ## x2 -11.26284 x = gradientDescent(X, y) 4.5 Optimised version optim(c(0,0,0), cost, gradient, X, y) ## $par ## [1] -25.1647048 0.2062524 0.2015046 ## ## $value ## [1] 0.2034977 ## ## $counts ## function gradient ## 156 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 4.6 Regularisation Add a penalty to each theta to avoid overfitting \\[J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\large[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))\\large] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2\\] The gradient/partial derivative therefore becomes \\[\\frac{1}{m} \\sum_{i=1}^m h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j\\] costr = function(theta,X,y,lambda=3) { m=length(y) z = X %*% theta h = g(z) cost = 1/m * (t(-y) %*% log(h) - (t(1-y) %*% log(1-h))) theta = theta^2 theta[1] = 0 # dont&#39;t penalty the bias theta cost = cost + lambda/(2*m) * sum(theta^2) return(cost) } gradientr = function(theta,X,y,lambda=3) { m=length(y) gradient = 1/m * t(X) %*% (g(X %*% theta) - y) theta[1] = 0 # don&#39;t include penalty term for bias theta gradient = gradient + (lambda / m) * theta return(gradient) } gradientDescentReg = function(X, y, theta=rep(0, ncol(X)), alpha=0.1, lambda=3, num_iters=4000) { m = length(y) J_history = rep(0, num_iters+1) theta_history = matrix(nrow=num_iters+1, ncol=ncol(X)) J_history[1] = costr(theta, X, y, lambda) theta_history[1, ] = theta for (iter in 1:num_iters) { deltas = alpha * gradientr(theta, X, y, lambda) theta = theta - deltas J_history[iter+1] = costr(theta, X, y, lambda) theta_history[iter+1, ] = theta } return(list(theta=theta, theta_history = theta_history, J_history=J_history)) } 4.7 Prediction predictlr = function(theta, X) g(X %*% theta) 4.7.1 Decision Boundary # mylr=gradientDescentReg(X,y) mylr=optim(c(0,0,0), costr, gradientr, X, y)$par dbgrid = expand.grid(x=20:100, y=20:100) dbgrid$pr = apply(dbgrid, 1, function(x) predictlr(mylr, cbind(1, x[1], x[2]))) ggplot(ex2data1, aes(x=V1, y=V2)) + geom_point(size=3, aes(pch=as.factor(V3), col=as.factor(V3))) + geom_contour(data=dbgrid, aes(x=x,y=y,z=pr), breaks=c(0,0.5)) + theme(legend.position = &quot;none&quot;) "]
]
