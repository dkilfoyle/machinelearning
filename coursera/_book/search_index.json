[
["index.html", "Machine Learning 1 Introduction", " Machine Learning Dean Kilfoyle 16 September 2017 1 Introduction Hello "],
["linear-regression.html", "2 Linear Regression 2.1 Model representation 2.2 Cost Function 2.3 Gradient Descent 2.4 Demo", " 2 Linear Regression library(ggplot2) library(latex2exp) library(shiny) ex1data1 = read.csv(&quot;ex1/ex1data1.txt&quot;, header=FALSE) names(ex1data1)=c(&quot;X&quot;,&quot;y&quot;) X = matrix(c(rep(1, nrow(ex1data1)), ex1data1$X), nrow=nrow(ex1data1), byrow=F) y = ex1data1$y theta=matrix(c(0,0),nrow=2) 2.1 Model representation \\[h_{\\theta}\\left(x\\right)\\ =\\ \\theta_0+\\theta_1\\cdot x\\] Where \\(\\theta_0\\) is the y intercept and \\(\\theta_1\\) is the gradient of the straight line fit. ggplot(ex1data1, aes(x=X, y=y)) + geom_point() + xlim(0,NA) + geom_smooth(method=&quot;lm&quot;, se=F, fullrange=T) lm(y~X) ## ## Call: ## lm(formula = y ~ X) ## ## Coefficients: ## (Intercept) X1 X2 ## -3.896 NA 1.193 2.2 Cost Function \\[J\\left(\\theta_0,\\theta_1\\right)=\\frac{1}{2\\cdot m}\\sum_{_{i=1}}^m\\left(h_{\\theta}\\left(x_i\\right)-y_i\\right)^2\\] Which is the sum of the squared vertical distance of each point from the straight line. m=lm(y~X) yhat=m$fitted.values diff=y-yhat ggplot(ex1data1, aes(x=X, y=y)) + geom_point() + geom_line(aes(x=X, y=yhat)) + geom_segment(aes(x=X, xend=X, y=y, yend=yhat, color=&quot;error&quot;)) J = function(X,y,theta) { m=length(y) ho = X %*% theta return(1/(2*m) * sum((ho-y)^2)) } 2.3 Gradient Descent The aim is to minimize the cost function. eg in 1 dimension (keeping theta0 = 0) xs = seq(from=0,to=1.5,by=0.1) cost = sapply(xs, function(theta1) J(X,y,rbind(0,theta1))) qplot(x=xs,y=cost) + geom_line() + xlab(TeX(&quot;$\\\\theta_1$&quot;)) + ylab(TeX(&quot;$J\\\\left(\\\\theta_0=0, \\\\theta_1\\\\right)&quot;)) In 2 dimensions theta1s = seq(from=-1,to=4,length.out=20) theta0s = seq(from=-10,to=10, length.out=20) cost=matrix(nrow=20,ncol=20) for (i in 1:20) { for (j in 1:20) { cost[j,i] = J(X, y, rbind(theta0s[i], theta1s[j])) } } # persp(x=theta0s,y=theta1s, cost, theta=-60, axes=T) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout # plot_ly(x=theta0s, y=theta1s, z=cost, type=&quot;scatter3d&quot;, mode=&#39;markers+lines&#39;) plot_ly(x=theta0s, y=theta1s, z=cost, type=&quot;surface&quot;) %&gt;% add_trace(x=c(5,0,-5), y=c(3,2,1), z=c(500,500,500), type=&quot;scatter3d&quot;, mode=&quot;lines&quot;) # contour(x=theta0s, y=theta1s, z=cost, levels=c(5,7,10,20,30,40,50,100,200,300)) theta1s = seq(from=-1,to=4,length.out=20) theta0s = seq(from=-10,to=10, length.out=20) cost=rbind() for (i in 1:20) { for (j in 1:20) { cost=rbind(cost, c(theta0s[i], theta1s[j], J(X, y, rbind(theta0s[i], theta1s[j])))) } } colnames(cost)=c(&quot;Th0&quot;,&quot;Th1&quot;,&quot;J&quot;) ggplot(as.data.frame(cost), aes(Th0, Th1, z=J)) + geom_contour(aes(color=..level..), binwidth=10) 2.3.1 Delta theta Delta theta is calculated from the partial derivative of the cost function \\[\\theta_j\\ =\\ \\theta_j\\ -\\ \\alpha\\cdot\\frac{\\partial}{\\partial\\theta_j}J\\left(\\theta_0,\\theta_1\\right)\\] The partial derivative can be calculated \\[ \\begin{align} \\frac{\\partial}{\\partial\\theta_j}J\\left(\\theta\\right)\\ &amp;= \\frac{\\partial}{\\partial\\theta_j}\\cdot\\frac{1}{2}\\cdot\\left(h_{\\theta}\\left(x\\right)-y\\right)^2 \\\\ &amp;= \\frac{1}{2}\\cdot\\left(h_{\\theta}\\left(x\\right)-y\\right)\\cdot\\frac{\\partial}{\\partial\\theta_j}\\left(h_{\\theta}\\left(x\\right)-y\\right) \\\\ &amp;= \\frac{1}{2}\\cdot\\left(h_{\\theta}\\left(x\\right)-y\\right)\\cdot\\frac{\\partial}{\\partial\\theta_j}\\left(h_{\\theta}\\left(x\\right)-y\\right) \\\\ &amp;= \\left(h_{\\theta}\\left(x\\right)-y\\right)\\cdot\\frac{\\partial}{\\partial\\theta_j}\\left(\\sum_{_{i=1}}^n\\theta_i\\cdot x_i-y\\right) \\\\ &amp;= \\left(h_{\\theta}\\left(x\\right)-y\\right)\\cdot x_j \\end{align} \\] Thus the delta equations using the solved partial derivative are \\[ \\theta_{0\\ }:=\\theta_0-\\alpha\\cdot\\frac{1}{m}\\cdot\\sum_{_{i=1}}^m\\left(h_{\\theta}\\left(x_i\\right)-y_i\\right)\\] Because \\(x_i\\) is 1 for \\(\\theta_0\\) \\[\\theta_1:=\\theta_1-\\alpha\\cdot\\frac{1}{m}\\cdot\\sum_{_{i=1}}^m\\left(\\left(h_{\\theta}\\left(x_i\\right)-y_i\\right)\\cdot x_i\\right)\\] 2.3.2 Code gradientDescent = function(X, y, theta=c(0,0), alpha=0.01, num_iters=1500) { m = length(y) J_history = rep(0, num_iters+1) theta_history = matrix(nrow=num_iters+1, ncol=2) J_history[1] = J(X, y, theta) theta_history[1, ] = t(theta) for (iter in 1:num_iters) { delta0 = alpha * (1/m) * sum(((X %*% theta) - y) * X[,1]) delta1 = alpha * (1/m) * sum(((X %*% theta) - y) * X[,2]) theta = theta - rbind(delta0, delta1) J_history[iter+1] = J(X, y, theta) theta_history[iter+1, ] = t(theta) } return(list(theta=theta, theta_history = theta_history, J_history=J_history)) } 2.4 Demo ## PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable. Shiny applications not supported in static R Markdown documents "],
["multiple-linear-regression.html", "3 Multiple Linear Regression 3.1 Model representation 3.2 Cost 3.3 Gradient Descent 3.4 Demo", " 3 Multiple Linear Regression library(ggplot2) library(latex2exp) library(shiny) ex1data2 = as.matrix(read.csv(&quot;ex1/ex1data2.txt&quot;, header=FALSE)) m=nrow(ex1data2) X = cbind(rep(1, m), ex1data2[, 1:2]) colnames(X) = c(&quot;x0&quot;,&quot;x1&quot;,&quot;x2&quot;) y = ex1data2[,3] theta=rep(0, ncol(X)) 3.1 Model representation \\[h_{\\theta}\\left(x\\right)\\ =\\ \\theta_0+\\theta_1 x_1 +\\theta_2 x_2\\] ## Normalization \\[ \\frac{x_i - u_i}{s_i}\\] X[,2:3]=apply(X[,2:3], 2, function(x) (x-mean(x))/sd(x)) 3.2 Cost J = function(X,y,theta) { m=length(y) ho = X %*% theta return(1/(2*m) * sum((ho-y)^2)) } 3.3 Gradient Descent gradientDescent = function(X, y, theta=rep(0, ncol(X)), alpha=0.01, num_iters=1500) { m = length(y) J_history = rep(0, num_iters+1) theta_history = matrix(nrow=num_iters+1, ncol=ncol(X)) J_history[1] = J(X, y, theta) theta_history[1, ] = theta for (iter in 1:num_iters) { deltas = apply(X,2, function(xi) { # where xi is the ith feature of X alpha * (1/m) * sum(((X %*% theta) - y) * xi) }) theta = theta - deltas J_history[iter+1] = J(X, y, theta) theta_history[iter+1, ] = theta } return(list(theta=theta, theta_history = theta_history, J_history=J_history)) } 3.4 Demo Shiny applications not supported in static R Markdown documents "],
["logistic-regression.html", "4 Logistic Regression 4.1 Model representation 4.2 Cost 4.3 Gradient Descent 4.4 Exercise 4.5 Optimised version 4.6 Regularisation 4.7 Prediction", " 4 Logistic Regression library(ggplot2) library(latex2exp) library(shiny) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union source(&quot;utils.R&quot;) 4.1 Model representation \\[\\begin{align*}&amp; h_\\theta (x) = g ( \\theta^T x ) \\newline \\newline&amp; z = \\theta^T x \\newline&amp; g(z) = \\dfrac{1}{1 + e^{-z}}\\end{align*}\\] ## Sigmoid Function g = function(z) { 1 / (1 + exp(-z)) } data.frame(x=seq(from=-5, to=5, by=0.1)) %&gt;% mutate(y=g(x)) %&gt;% ggplot(aes(x,y)) + geom_line() + geom_vline(xintercept = 0, linetype=2) 4.2 Cost \\[\\begin{align*}&amp; J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)}) \\newline &amp; \\mathrm{Cost}(h_\\theta(x),y) = -\\log(h_\\theta(x)) \\; &amp; \\text{if y = 1} \\newline &amp; \\mathrm{Cost}(h_\\theta(x),y) = -\\log(1-h_\\theta(x)) \\; &amp; \\text{if y = 0}\\end{align*}\\] Which can be expressed in a single equation as \\[\\mathrm{Cost}(h_\\theta(x),y) = - y \\; \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x))\\] ## Loading required package: grid 4.2.1 Gradient of Cost Function Is the partial derivative and is identical to the linear regression derivative \\[\\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\\] 4.2.2 Vectorized \\[\\begin{align*} h &amp;= g(X\\theta)\\newline J(\\theta) &amp;= \\frac{1}{m} \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right)\\newline \\dfrac{\\partial}{\\partial \\theta_J} &amp;= \\frac{1}{m} X^{T} (g(X \\theta ) - \\vec{y}) \\end{align*}\\] cost = function(theta, X, y) { m=length(y) z = X %*% theta h = g(z) 1/m * (t(-y) %*% log(h) - (t(1-y) %*% log(1-h))) } gradient = function(theta, X, y) { 1/m * t(X) %*% (g(X %*% theta) - y) } 4.3 Gradient Descent Calculating the partial derivative of the above cost function gives an identical equation to the multiple linear regression delta: \\[ \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\\] ### Vectorized \\[\\theta := \\theta - \\frac{\\alpha}{m} X^{T} (g(X \\theta ) - \\vec{y})\\] gradientDescent = function(X, y, theta=rep(0, ncol(X)), alpha=0.1, num_iters=4000) { m = length(y) J_history = rep(0, num_iters+1) theta_history = matrix(nrow=num_iters+1, ncol=ncol(X)) J_history[1] = cost(theta, X, y) theta_history[1, ] = theta alpha_over_m = alpha / m for (iter in 1:num_iters) { deltas = alpha_over_m * t(X) %*% (g(X %*% theta) - y) theta = theta - deltas J_history[iter+1] = cost(theta, X, y) theta_history[iter+1, ] = theta } return(list(theta=theta, theta_history = theta_history, J_history=J_history)) } 4.4 Exercise X = as.matrix(cbind(rep(1, m), ex2data1[, 1:2])) colnames(X) = c(&quot;x0&quot;,&quot;x1&quot;,&quot;x2&quot;) n=ncol(X) y = ex2data1[,3] theta=rep(0, n) cost(theta, X, y) ## [,1] ## [1,] 0.6931472 gradient(theta,X,y) ## [,1] ## x0 -0.10000 ## x1 -12.00922 ## x2 -11.26284 x = gradientDescent(X, y) 4.5 Optimised version optim(c(0,0,0), cost, gradient, X, y) ## $par ## [1] -25.1647048 0.2062524 0.2015046 ## ## $value ## [1] 0.2034977 ## ## $counts ## function gradient ## 156 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 4.6 Regularisation Add a penalty to each theta to avoid overfitting \\[J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\large[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))\\large] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2\\] The gradient/partial derivative therefore becomes \\[\\frac{1}{m} \\sum_{i=1}^m h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j\\] costr = function(theta,X,y,lambda=3) { m=length(y) z = X %*% theta h = g(z) cost = 1/m * (t(-y) %*% log(h) - (t(1-y) %*% log(1-h))) theta[1] = 0 # dont&#39;t penalty the bias theta cost = cost + lambda/(2*m) * sum(theta^2) return(cost) } gradientr = function(theta,X,y,lambda=3) { m=length(y) gradient = 1/m * t(X) %*% (g(X %*% theta) - y) theta[1] = 0 # don&#39;t include penalty term for bias theta gradient = gradient + (lambda / m) * theta return(gradient) } gradientDescentReg = function(X, y, theta=rep(0, ncol(X)), alpha=0.1, lambda=3, num_iters=4000) { m = length(y) J_history = rep(0, num_iters+1) J_history[1] = costr(theta, X, y, lambda) for (iter in 1:num_iters) { deltas = alpha * gradientr(theta, X, y, lambda) theta = theta - deltas J_history[iter+1] = costr(theta, X, y, lambda) } J_history[is.nan(J_history)] = NA return(list(theta=theta, J_history=J_history)) } 4.7 Prediction predictlr = function(theta, X) g(X %*% theta) 4.7.1 Decision Boundary # mylr=gradientDescentReg(X,y) mylr=optim(c(0,0,0), costr, gradientr, X, y, method=&quot;BFGS&quot;)$par dbgrid = expand.grid(x=20:100, y=20:100) dbgrid$pr = apply(dbgrid, 1, function(x) predictlr(mylr, cbind(1, x[1], x[2]))) ggplot(ex2data1, aes(x=V1, y=V2)) + geom_point(size=3, aes(pch=as.factor(V3), col=as.factor(V3))) + geom_contour(data=dbgrid, aes(x=x,y=y,z=pr), breaks=c(0,0.5)) + theme(legend.position = &quot;none&quot;) shinyApp( ui = fluidPage( inputPanel( numericInput(&quot;num_iters&quot;, &quot;Num Iterations&quot;, 400, min=10, max=10000, step=100), numericInput(&quot;alpha&quot;, &quot;Alpha&quot;, 0.1, min=0, max=1.0, step=0.1), numericInput(&quot;lambda&quot;, &quot;Lambda&quot;,0, min=0, max=1000, step=1) ), fluidRow( column(width=4, plotOutput(&quot;costPlot&quot;)), column(width=4, plotOutput(&quot;boundaryPlot&quot;)), column(width=4, h4(&quot;Gradient Descent Theta&quot;), textOutput(&quot;gdresults&quot;), h4(&quot;Optim Theta&quot;), textOutput(&quot;optimresults&quot;)) ) ), server = function(session, input, output) { solve_theta = reactive({ gradientDescentReg(X, y, theta=c(0,0,0), alpha=input$alpha, lambda=input$lambda, num_iters=input$num_iters) }) output$costPlot = renderPlot({ mylr = solve_theta() ggplot(data.frame(x=1:length(mylr$J_history), y=mylr$J_history)) + geom_point(aes(x, y)) }) output$boundaryPlot &lt;- renderPlot({ optimlr = optim(c(0,0,0), costr, gradientr, X, y)$par mylr=solve_theta()$theta dbgrid = expand.grid(x=20:100, y=20:100) dbgrid$optimpr = apply(dbgrid, 1, function(x) predictlr(optimlr, cbind(1, x[1], x[2]))) dbgrid$gdpr = apply(dbgrid, 1, function(x) predictlr(mylr, cbind(1, x[1], x[2]))) ggplot(ex2data1, aes(x=V1, y=V2)) + geom_point(size=3, aes(pch=as.factor(V3), col=as.factor(V3))) + geom_contour(data=dbgrid, aes(x=x,y=y,z=optimpr), breaks=c(0,0.5)) + geom_contour(data=dbgrid, aes(x=x,y=y,z=gdpr), breaks=c(0,0.5)) + theme(legend.position = &quot;none&quot;) }) output$gdresults = renderText({ solve_theta()$theta }) output$optimresults = renderText({ optim(c(0,0,0), costr, gradientr, X, y)$par }) }, options = list(height = 500) ) Shiny applications not supported in static R Markdown documents "],
["neural-network.html", "5 Neural Network 5.1 Model Representation 5.2 Exercise", " 5 Neural Network library(ggplot2) library(latex2exp) library(shiny) library(dplyr) 5.1 Model Representation neuralnetwork.png 5.2 Exercise ## R.matlab v3.6.1 (2016-10-19) successfully loaded. See ?R.matlab for help. ## ## Attaching package: &#39;R.matlab&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## getOption, isOpen df = expand.grid(r=1:20,c=1:20) df$z = ex3data1$X[2500,] ggplot(df, aes(c,-r,fill=z)) + geom_raster()+ coord_fixed() g = function(z) { 1 / (1 + exp(-z)) } costr = function(theta,X,y,lambda=3) { m=length(y) z = X %*% theta h = g(z) cost = 1/m * (t(-y) %*% log(h) - (t(1-y) %*% log(1-h))) theta[1] = 0 # dont&#39;t penalty the bias theta cost = cost + lambda/(2*m) * sum(theta^2) return(cost) } gradientr = function(theta,X,y,lambda=3) { m=length(y) gradient = 1/m * t(X) %*% (g(X %*% theta) - y) theta[1] = 0 # don&#39;t include penalty term for bias theta gradient = gradient + (lambda / m) * theta return(gradient) } predictlr = function(theta, X) g(X %*% theta) theta_t = c(-2, -1, 1, 2) X_t = cbind(rep(1,5), matrix(1:15,nrow=5)/10) y_t = c(1,0,1,0,1) lambda_t = 3 costr(theta_t, X_t, y_t, lambda_t) ## [,1] ## [1,] 2.534819 gradientr(theta_t, X_t, y_t, lambda_t) ## [,1] ## [1,] 0.1465614 ## [2,] -0.5485584 ## [3,] 0.7247223 ## [4,] 1.3980030 lambda = 0.1 m = nrow(ex3data1$X) n = ncol(ex3data1$X) num_labels = 10 oneVsAll = function(X, y, num_labels, lambda) { thetas = matrix(0, nrow=num_labels, ncol=n+1) X = cbind(rep(1,m), X) # add a column of 1s for X_0 initial_theta = rep(0, n+1) for (i in 1:num_labels) { # train y=i vs not i y_t = matrix(as.integer(y==i),ncol=1) thetas[i, ] = optim(initial_theta, costr, gradientr, X, y_t, lambda, method=&quot;BFGS&quot;, control=list(maxit=50))$par } return(thetas) } thetas = oneVsAll(ex3data1$X, ex3data1$y, num_labels, lambda) predictOneVsAll = function(thetas, X) { m = nrow(X) #5000 num_labels = nrow(thetas) #10 X = cbind(rep(1,m), X) # add a column of 1s for X_0 #t(thetas) = # 1 2 3 4 5 6 7 8 9 10 # . . . . . . . . . . # . . . . . . . . . . # 400 p = g(X %*% t(thetas)) # each column of p is the predicted probability that y=1 for that column for each row of X return(apply(p, 1, which.max)) } p = predictOneVsAll(thetas, ex3data1$X) mean(p==ex3data1$y) ## [1] 0.9318 "]
]
