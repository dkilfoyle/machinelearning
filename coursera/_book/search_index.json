[
["bias-and-variance.html", "7 Bias and Variance 7.1 Definitions 7.2 Learning Curve 7.3 Example linear regression 7.4 Learning Curve 7.5 Polynomial features 7.6 Optimise Lambda", " 7 Bias and Variance 7.1 Definitions Training set 60% Optimise theta for the current model design Cross validation set 20% Check the error for the current thetas Use this error to find the optimum polynomial degree Test set 20% Check the error to assess the generalization 7.1.1 High bias is under-fitting Training and Cross validation errors are high Add more polynomial degrees or more features Adding more samples may not help 7.1.2 High variance is over-fitting Training error is low but cross validation error is high Reducing features or polynomial degress will help Adding more samples will help 7.1.3 Polynomial Adding more polynomial features will improve high-bias underfitting. Adding too many polynomial features will result in high variance over-fitting 7.2 Learning Curve 7.3 Example linear regression ## &lt;environment: R_GlobalEnv&gt; 7.3.1 Regularised Linear Regression Cost \\[J(\\Theta) = \\dfrac{1}{2m} \\bigg( \\sum_{i=1}^m(h_\\Theta(x^{(i)}) - y^{(i)})^2 \\bigg) + \\dfrac{\\lambda}{2m} \\bigg( \\sum_{j=1}^{n}\\theta^2 \\bigg)\\] 7.3.2 Regularised Linear Regression Gradient \\[\\frac{\\partial J\\left(\\theta\\right)}{\\partial\\theta_j} =\\dfrac{1}{m} \\bigg( \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)} \\bigg) + \\dfrac{\\lambda}{m}\\theta_j\\] 7.3.3 Code addBias = function(X) { return(cbind(rep(1, nrow(X)), X)) } linearRegCostFunction = function(theta, X, y, lambda=0) { # X should have a column of 1s at the start for bias m = length(y) theta = as.vector(theta) ho = X %*% theta J = (1/(2*m) * sum((ho-y)^2)) theta[1] = 0 # dont&#39;t penalty the bias theta J = J + (lambda / (2 * m)) * sum(theta^2) return(J) } linearRegGradFunction = function(theta, X, y, lambda=0) { # X should have a column of 1s at the start for bias theta = as.vector(theta) m=length(y) gradient = 1/m * t(X) %*% ((X %*% theta) - y) theta[1] = 0 # don&#39;t include penalty term for bias theta gradient = gradient + (lambda / m) * theta return(gradient) } trainLinearReg = function(X, y, lambda) { initial_theta = matrix(0, ncol(X), 1) mylr=optim(initial_theta, linearRegCostFunction, linearRegGradFunction, X, y, lambda, method=&quot;BFGS&quot;) return(mylr$par) } predict.lr = function(theta, X) { addBias(X) %*% theta } thetas = trainLinearReg(addBias(X),y,0) data.frame(X=X,y=y) %&gt;% ggplot(aes(X,y)) + geom_point() + geom_abline(intercept=thetas[1,1], slope=thetas[2,1]) 7.4 Learning Curve learningCurve = function(X, y, Xval, yval, lambda) { # X and Xval should have bias columns m = nrow(X) error_train = matrix(0, nrow=m, ncol=1) error_val = matrix(0, nrow=m, ncol=1) for (i in 1:m) { # train the thetas using 1:i examples Xi = matrix(X[1:i,],nrow=i) yi = matrix(y[1:i,],nrow=i) thetas = trainLinearReg(Xi, yi, lambda) error_train[i] = linearRegCostFunction(thetas, Xi, yi, lambda=0) error_val[i] = linearRegCostFunction(thetas, Xval, yval, lambda=0) } return(list(error_train=error_train, error_val=error_val)) } plotLearningCurve = function(lc) { data.frame(x=1:m, train=lc$error_train, validate=lc$error_val) %&gt;% gather(key=run, value=error, -x) %&gt;% ggplot(aes(x=as.integer(x), y=error)) + geom_line(aes(group=run, color=run)) + xlab(&quot;Numer of training examples&quot;) + theme(legend.position = c(0.8, 0.8)) } lc = learningCurve(addBias(X), y, addBias(Xval), yval, 0) plotLearningCurve(lc) 7.5 Polynomial features \\[h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3 + \\theta_4 x_1^4 + \\theta_5 x_1^5 ...\\] Convert X into a matrix with x1, x1^2, x1^3, x1^4 etc. Then use scale so that each column is centered with mean=0, sd=1 polyFeatures = function(X, p) { XX =matrix(0, ncol=p, nrow=nrow(X)) for (i in 1:p) { XX[,i] = X[,1]^i } return(XX) } Example using polynomial degree 8 - this overfits the data resulting in a low training error but high validation error. plotPolyFit = function(minx, maxx, mu, sigma, theta, p) { xr = seq(minx,maxx,by=(maxx-minx)/100) Xr = scale(polyFeatures(cbind(xr),p), center=mu, scale=sigma) Xf = data.frame(X=xr, y=predict.lr(theta, Xr)) geom_line(data=Xf, aes(X,y), linetype=&quot;dashed&quot;, color=&quot;blue&quot;) } X_poly = scale(polyFeatures(X,8)) thetas = trainLinearReg(addBias(X_poly),y,lambda=0) fit.plot=ggplot(data.frame(x=X,y=y), aes(x,y)) + geom_point(color=&quot;red&quot;) + plotPolyFit(min(X),max(X), mu=attr(X_poly,&quot;scaled:center&quot;), sigma=attr(X_poly,&quot;scaled:scale&quot;), thetas, 8) X_poly_val = scale(polyFeatures(Xval,8), center=attr(X_poly, &quot;scaled:center&quot;), scale=attr(X_poly, &quot;scaled:scale&quot;)) lc = learningCurve(addBias(X_poly), y, addBias(X_poly_val), yval, 0) multiplot(fit.plot, plotLearningCurve(lc),cols = 2) ## Loading required package: grid Fix the over-fitting by adding lambda = 1 thetas = trainLinearReg(addBias(X_poly),y,lambda=1) fit.plot=ggplot(data.frame(x=X,y=y), aes(x,y)) + geom_point(color=&quot;red&quot;) + plotPolyFit(min(X),max(X), mu=attr(X_poly,&quot;scaled:center&quot;), sigma=attr(X_poly,&quot;scaled:scale&quot;), thetas, 8) X_poly_val = scale(polyFeatures(Xval,8), center=attr(X_poly, &quot;scaled:center&quot;), scale=attr(X_poly, &quot;scaled:scale&quot;)) lc = learningCurve(addBias(X_poly), y, addBias(X_poly_val), yval, lambda=1) multiplot(fit.plot, plotLearningCurve(lc),cols = 2) 7.6 Optimise Lambda Choose the lambda with the lowest validation error lambdas = c(0,0.001,0.003,0.01,0.03,0.1,0.3,1,3,10) train_costs =c() val_costs = c() for (lambda in lambdas) { thetas = trainLinearReg(addBias(X_poly),y,lambda) # train theta with training set train_costs = c(train_costs, linearRegCostFunction(thetas, addBias(X_poly), y, lambda=0)) val_costs = c(val_costs, linearRegCostFunction(thetas, addBias(X_poly_val), yval, lambda=0)) } data.frame(x=factor(1:10,labels=lambdas), train=train_costs, val=val_costs) %&gt;% gather(key=run, val=cost, -x) %&gt;% ggplot(aes(x=x,y=cost,group=run,color=run)) + geom_line() "]
]
