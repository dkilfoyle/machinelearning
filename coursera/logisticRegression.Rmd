---
title: "Logistic Regression"
output:  html_document
runtime: shiny
---

# Logistic Regression

```{r }
library(ggplot2)
library(latex2exp)
library(shiny)
library(dplyr)
source("utils.R")
```


## Model representation

$$\begin{align*}& h_\theta (x) = g ( \theta^T x ) \newline \newline& z = \theta^T x \newline& g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}$$
## Sigmoid Function

```{r}
g = function(z) {
  1 / (1 + exp(-z))
}
```


```{r}
data.frame(x=seq(from=-5, to=5, by=0.1)) %>% 
  mutate(y=g(x)) %>% 
  ggplot(aes(x,y)) + geom_line() + geom_vline(xintercept = 0, linetype=2)
```

## Cost

$$\begin{align*}& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1} \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}\end{align*}$$

Which can be expressed in a single equation as


$$\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))$$

```{r echo=F}
p1 = data.frame(x=seq(from=0, to=1, by=0.01)) %>% 
  mutate(y=-log(x)) %>% 
  ggplot(aes(x,y)) + geom_line() + ggtitle("If y=1") + ylab(TeX("$J (\\theta)$")) + xlab(TeX("$h_{\\theta}(x)$"))
p2 = data.frame(x=seq(from=0, to=1, by=0.01)) %>% 
  mutate(y=-log(1-x)) %>% 
  ggplot(aes(x,y)) + geom_line() + ggtitle("If y=0") + ylab(TeX("$J (\\theta)$")) + xlab(TeX("$h_{\\theta}(x)$"))
multiplot(p1,p2,cols = 2)

```

### Gradient of Cost Function

Is the partial derivative and is identical to the linear regression derivative

$$\dfrac{\partial}{\partial \theta_j}J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$


### Vectorized

$$\begin{align*} h &= g(X\theta)\newline J(\theta) &= \frac{1}{m} \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)\newline  \dfrac{\partial}{\partial \theta_J} &= \frac{1}{m} X^{T} (g(X \theta ) - \vec{y}) \end{align*}$$


```{r costFunction.m}
cost = function(theta, X, y) {
  m=length(y)
  z = X %*% theta
  h = g(z)
  1/m * (t(-y) %*% log(h) - (t(1-y) %*% log(1-h)))
}
gradient = function(theta, X, y) {
  1/m * t(X) %*% (g(X %*% theta) - y)
}
```

## Gradient Descent

Calculating the partial derivative of the above cost function gives an identical equation to the multiple linear regression delta:

$$ \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$
### Vectorized

$$\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})$$


```{r gradientDesc.m}

gradientDescent = function(X, y, theta=rep(0, ncol(X)), alpha=0.1, num_iters=4000) {
  m = length(y)
  J_history = rep(0, num_iters+1)
  theta_history = matrix(nrow=num_iters+1, ncol=ncol(X))
  
  J_history[1] = cost(theta, X, y)
  theta_history[1, ] = theta
  
  alpha_over_m = alpha / m
  
  for (iter in 1:num_iters) {
    deltas = alpha_over_m * t(X) %*% (g(X %*% theta) - y)
    theta = theta - deltas
    J_history[iter+1] = cost(theta, X, y)
    theta_history[iter+1, ] = theta
  }
  
  return(list(theta=theta, theta_history = theta_history, J_history=J_history))
}
```

## Exercise

```{r, echo = FALSE}
ex2data1 = read.csv("ex2/ex2data1.txt", header=FALSE)
m=nrow(ex2data1)
ggplot(ex2data1, aes(x=V1,y=V2,color=as.factor(V3))) + geom_point(size=3, aes(pch=as.factor(V3), col=as.factor(V3))) + theme(legend.position="none")
```

```{r}
X = as.matrix(cbind(rep(1, m), ex2data1[, 1:2]))
colnames(X) = c("x0","x1","x2")
n=ncol(X)
y = ex2data1[,3]
theta=rep(0, n)
```

```{r}
cost(theta, X, y)
```

```{r}
gradient(theta,X,y)
```


```{r}
x = gradientDescent(X, y)
```

## Optimised version

```{r}
optim(c(0,0,0), cost, gradient, X, y)
```

## Regularisation

Add a penalty to each theta to avoid overfitting

$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$
The gradient/partial derivative therefore becomes

$$\frac{1}{m} \sum_{i=1}^m h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j$$

```{r}
costr = function(theta,X,y,lambda=3) {
  m=length(y)
  z = X %*% theta
  h = g(z)
  cost = 1/m * (t(-y) %*% log(h) - (t(1-y) %*% log(1-h)))
  theta = theta^2
  theta[1] = 0 # dont't penalty the bias theta
  cost = cost + lambda/(2*m) * sum(theta^2)
  return(cost)
}
gradientr = function(theta,X,y,lambda=3) {
  m=length(y)
  gradient = 1/m * t(X) %*% (g(X %*% theta) - y)
  theta[1] = 0 # don't include penalty term for bias theta
  gradient = gradient + (lambda / m) * theta
  return(gradient)
}
```

```{r}
gradientDescentReg = function(X, y, theta=rep(0, ncol(X)), alpha=0.1, lambda=3, num_iters=4000) {
  m = length(y)
  J_history = rep(0, num_iters+1)
  J_history[1] = costr(theta, X, y, lambda)
  
  for (iter in 1:num_iters) {
    deltas = alpha * gradientr(theta, X, y, lambda)
    theta = theta - deltas
    J_history[iter+1] = costr(theta, X, y, lambda)
  }
  
  J_history[is.nan(J_history)] = NA
  
  return(list(theta=theta, J_history=J_history))
}
```

## Prediction

```{r}
predictlr = function(theta, X) g(X %*% theta)
```

### Decision Boundary

```{r}
# mylr=gradientDescentReg(X,y)
mylr=optim(c(0,0,0), costr, gradientr, X, y)$par

dbgrid = expand.grid(x=20:100, y=20:100)
dbgrid$pr = apply(dbgrid, 1, function(x) predictlr(mylr, cbind(1, x[1], x[2])))

ggplot(ex2data1, aes(x=V1, y=V2)) + 
  geom_point(size=3, aes(pch=as.factor(V3), col=as.factor(V3))) +
  geom_contour(data=dbgrid, aes(x=x,y=y,z=pr), breaks=c(0,0.5)) +
  theme(legend.position = "none")

```

```{r}
shinyApp(
  
  ui = fluidPage(
    inputPanel(
      numericInput("num_iters", "Num Iterations", 400, min=10, max=10000, step=100),
      numericInput("alpha", "Alpha", 0.1, min=0, max=1.0, step=0.1),
      numericInput("lambda", "Lambda",0, min=0, max=1000, step=1)
    ),
    fluidRow(
      column(width=4, plotOutput("costPlot")),
      column(width=4, plotOutput("boundaryPlot")),
      column(width=4, 
        h4("Gradient Descent Theta"),
        textOutput("gdresults"),
        h4("Optim Theta"),
        textOutput("optimresults"))
    )
  ),
  
  server = function(session, input, output) {
    
    solve_theta = reactive({
      gradientDescentReg(X, y, theta=c(0,0,0), alpha=input$alpha, lambda=input$lambda, num_iters=input$num_iters)
    })
    
    output$costPlot = renderPlot({
      mylr = solve_theta()
      ggplot(data.frame(x=1:length(mylr$J_history), y=mylr$J_history)) +
        geom_point(aes(x, y))
      
    })
    
    output$boundaryPlot <- renderPlot({
      optimlr = optim(c(0,0,0), costr, gradientr, X, y)$par
      mylr=solve_theta()$theta
      
      dbgrid = expand.grid(x=20:100, y=20:100)
      dbgrid$optimpr = apply(dbgrid, 1, function(x) predictlr(optimlr, cbind(1, x[1], x[2])))
      dbgrid$gdpr = apply(dbgrid, 1, function(x) predictlr(mylr, cbind(1, x[1], x[2])))
      
      ggplot(ex2data1, aes(x=V1, y=V2)) + 
        geom_point(size=3, aes(pch=as.factor(V3), col=as.factor(V3))) +
        geom_contour(data=dbgrid, aes(x=x,y=y,z=optimpr), breaks=c(0,0.5)) +
        geom_contour(data=dbgrid, aes(x=x,y=y,z=gdpr), breaks=c(0,0.5)) +
        theme(legend.position = "none")
    })
    
    output$gdresults = renderText({
      solve_theta()$theta
    })
    
    output$optimresults = renderText({
      optim(c(0,0,0), costr, gradientr, X, y)$par
    })
  },
  
  options = list(height = 500)
)
```

