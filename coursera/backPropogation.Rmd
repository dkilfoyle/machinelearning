---
title: "Back Propogation"
author: "Dean Kilfoyle"
date: "16 September 2017"
output: html_document
---

# Back Propogation

```{r}
library(ggplot2)
library(latex2exp)
library(shiny)
library(dplyr)
```

## Cost Function

$$\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}$$
Where:
K is the number of output classes
m is the number of data samples in X
L is the number of layers
s_l is the number of units in layer l exluding the bias

```{r}
g = function(z) { 1 / (1 + exp(-z)) }

nnCost = function(thetas, input_layer_size, hidden_layer_size, num_labels, X, y, lambda=0) {
  
  # Reshape the unrolled thetas into the weight matrices
  theta1_size = hidden_layer_size * (input_layer_size + 1)
  theta1 = matrix(thetas[1:theta1_size], nrow=hidden_layer_size, ncol=(input_layer_size+1))
  theta2 = matrix(thetas[(theta1_size+1):length(thetas)], nrow=num_labels, ncol=(hidden_layer_size+1))

  # Setup some useful variables
  m = nrow(X)
  n = ncol(X)
  
  # Add a column of 1s to X for x_0
  X = cbind(rep(1,m),X)
         
  J = 0
  theta1_grad = matrix(0, nrow=nrow(theta1), ncol=ncol(theta1))
  theta2_grad = matrix(0, nrow=nrow(theta2), ncol=ncol(theta2))
  
  for (i in 1:m) {
    y_k = diag(num_labels)[y[i],]   # eg y=3 = c(0,0,1,0,0,0,0,0,0,0)
  
    a2 = g(theta1 %*% X[i, ]) # input to hidden
    a2 = rbind(1, a2) # add a_0^(2)
    h = g(theta2 %*% a2) # hidden to output
    
    # vectorise the sum over k labels
    cost = 1/m * (t(-y_k) %*% log(h) - (t(1-y_k) %*% log(1-h)))
    J = J + cost 
  }
  
  penalty = (lambda / (2 * m)) * (sum(theta1[,-1]^2) + sum(theta2[,-1]^2))
  J = J + penalty
  
  return(J)
}
```

## Sigmoid gradient

Gradient = derivative of the sigmoid function with respect to z

$$g'(z) = \frac{d}{dz}g(z) = g(z) (1-g(z))$$
and g(z) = a

$$g'(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})$$
```{r}
dgdz = function (z) {
  return (g(z)*(1-g(z)))
}
```


## Gradients

Aim is to minimize J(theta). As with linear and logistic regression need to calculate the gradient of the cost function

$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)$$
To calculate the gradients:

1. set $\Delta_{i,j}^{(l)}$ to 0 for all l,i,j
2. For i = 1 to m
  a) Forward propogate to compute a(l)
  b) Using y(i) compute 
$$\delta^{(L)} = a^{(L)} - y^{(i)}$$
  c) Compute deltas for earlier layers = a weighted average of the errors in l+1
$$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})$$
     where a(1-a) is the sigmoid derivative
  d) Update $\Delta$
$$\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$$

or vectorized

$$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$$



  
## Exercise

```{r, echo = FALSE}
library(R.matlab)
ex4data1 = readMat("ex4/ex4data1.mat")
ex4weights = readMat("ex4/ex4weights.mat")
```

```{r}
df = expand.grid(r=1:20,c=1:20)
df$z = ex4data1$X[2500,]
ggplot(df, aes(c,-r,fill=z)) + geom_raster()+ coord_fixed()
```


```{r}
input_layer_size  = 400  # 20x20 Input Images of Digits
hidden_layer_size = 25   # 25 hidden units
num_labels = 10
nn_params = c(as.vector(ex4weights$Theta1), as.vector(ex4weights$Theta2))
nnCost(nn_params, input_layer_size, hidden_layer_size, num_labels, ex4data1$X, ex4data1$y)
```

```{r}
nnCost(nn_params, input_layer_size, hidden_layer_size, num_labels, ex4data1$X, ex4data1$y, lambda=1)
```


